# Task 1: House Price Prediction using Linear Regression

# 1. Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from google.colab import files

# 2. Upload dataset manually
uploaded = files.upload()
data = pd.read_csv("train.csv")

print("Shape:", data.shape)
print(data.head())


# 2. Choose Features & Target

features = [
    "GrLivArea",      # above ground living area
    "TotalBsmtSF",    # total basement area
    "OverallQual",    # material/finish quality
    "YearBuilt",
    "FullBath",
    "BedroomAbvGr",
    "TotRmsAbvGrd",
    "Neighborhood"    # categorical feature
]

target = "SalePrice"
df = data[features + [target]].copy()

# 3. Train/Test Split
X = df.drop(columns=[target])
y = df[target]

X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. Preprocessing Pipeline
numeric_feats = X.select_dtypes(include=["int64", "float64"]).columns.tolist()
categorical_feats = X.select_dtypes(include=["object"]).columns.tolist()

print("Numeric feats:", numeric_feats)
print("Categorical feats:", categorical_feats)

numeric_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_feats),
        ("cat", categorical_transformer, categorical_feats)
    ]
)

# 5. Build & Train Model
model = Pipeline([
    ("preproc", preprocessor),
    ("regressor", LinearRegression())
])

model.fit(X_train, y_train)

# 6. Evaluate
y_pred = model.predict(X_valid)

mse = mean_squared_error(y_valid, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_valid, y_pred)

print("\nPerformance on validation set:")
print(f"RMSE = {rmse:.2f}")
print(f"RÂ² = {r2:.4f}")

# Cross-validation
scores = cross_val_score(model, X, y, scoring="neg_root_mean_squared_error", cv=5)
cv_rmse = -scores
print("Cross-validation RMSEs:", cv_rmse)
print("Average CV RMSE:", cv_rmse.mean())

# 7. Plot Actual vs Predicted
plt.figure(figsize=(8, 6))
plt.scatter(y_valid, y_pred, alpha=0.6, color="blue")
plt.xlabel("Actual SalePrice")
plt.ylabel("Predicted SalePrice")
plt.title("Actual vs Predicted SalePrice (Validation Set)")
lims = [min(y_valid.min(), y_pred.min()), max(y_valid.max(), y_pred.max())]
plt.plot(lims, lims, "--r")
plt.show()

# 8. Feature Coefficients
if isinstance(model.named_steps["regressor"], LinearRegression):
    ohe = model.named_steps["preproc"].named_transformers_["cat"].named_steps["onehot"]
    cat_feature_names = ohe.get_feature_names_out(categorical_feats)
    num_feature_names = numeric_feats
    all_feature_names = np.concatenate([num_feature_names, cat_feature_names])

    coefs = model.named_steps["regressor"].coef_
    coef_df = pd.DataFrame({
        "feature": all_feature_names,
        "coefficient": coefs
    }).sort_values(by="coefficient", key=lambda s: np.abs(s), ascending=False)

    print("\nTop coefficients:")
    print(coef_df.head(10))
